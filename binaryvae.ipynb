{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/jthickstun/gm-hw2\n",
    "wget https://courses.cs.washington.edu/courses/cse599i/20au/resources/data.tar.gz\n",
    "tar -xf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from time import time\n",
    "from torchvision import transforms\n",
    "from IPython import display\n",
    "\n",
    "sys.path.append('gm-hw2')\n",
    "import bmnist,losses,models\n",
    "\n",
    "root = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'   # see issue #152\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "def worker_init(args):\n",
    "    signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
    "    \n",
    "autoregress = False\n",
    "batch_size = 100\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True, 'worker_init_fn': worker_init}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = bmnist.BinarizedMNIST(root=root, train=True)\n",
    "test_set = bmnist.BinarizedMNIST(root=root, train=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,batch_size=batch_size,shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,batch_size=batch_size,shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_images(x, ax, n=7, d=28, color='black'):\n",
    "    ax.cla()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    panel = np.zeros([n*d,n*d])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            panel[i*d:(i+1)*d,j*d:(j+1)*d] = x[i*n+j,0]\n",
    "            \n",
    "    ax.imshow(panel, cmap=plt.get_cmap('Greys'))\n",
    "    plt.setp(ax.spines.values(), color=color)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "            \n",
    "for i, x in enumerate(train_loader):\n",
    "    print_images(x, ax)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = models.DiscreteVAEDecoder(autoregress=autoregress).cuda()\n",
    "f = models.VAEEncoder().cuda()\n",
    "loss_func = losses.discrete_output_elbo\n",
    "\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "loss_r = []\n",
    "loss_d = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10,18))\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(f.parameters())+list(g.parameters()), lr=1e-3)\n",
    "i = 0\n",
    "t0 = time()\n",
    "for epoch in range(60):\n",
    "    if autoregress and epoch == 10: g.autoregress = True # after warmup, start autoregressive conditioning\n",
    "    if epoch == 40: optimizer.param_groups[0]['lr'] = lr = 3e-4 # cut the learning rate\n",
    "    for x in train_loader:\n",
    "        x = x.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        epsilon = torch.randn(batch_size,1,7,7).cuda()\n",
    "        z, logqzx, _, _ = f(x, epsilon)\n",
    "        \n",
    "        recon,div = loss_func(g(z,x),x,z,logqzx)\n",
    "        loss = recon + div\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #\n",
    "        # book-keeping after this\n",
    "        #\n",
    "        \n",
    "        loss_train.append(loss.detach().cpu().numpy())\n",
    "        loss_r.append(recon.detach().cpu().numpy())\n",
    "        loss_d.append(div.detach().cpu().numpy())\n",
    "        \n",
    "        if i % 600 == 0:\n",
    "            g.eval(); f.eval()\n",
    "            fake = g.sample(torch.randn(batch_size,1,7,7).cuda())\n",
    "            display.clear_output(wait=True)\n",
    "            print_images(fake.detach().cpu(), ax[0], color='black')\n",
    "            print_images(x.cpu(), ax[2], color='green')\n",
    "            print_images(g.sample(z).detach().cpu(), ax[1], color='red')\n",
    "            display.display(plt.gcf())\n",
    "            \n",
    "            losst = 0\n",
    "            for x in test_loader:\n",
    "                x = x.cuda()\n",
    "                epsilon = torch.randn(batch_size,1,7,7).cuda()\n",
    "                z, logqzx, _, _ = f(x, epsilon)\n",
    "                \n",
    "                recont,divt = loss_func(g(z,x),x,z,logqzx)\n",
    "                losst += recont.detach() + divt.detach()\n",
    "            \n",
    "            losst = losst.cpu().numpy() / len(test_loader)\n",
    "            print(i,time()-t0,loss_train[-1],losst)\n",
    "            t0 = time()\n",
    "            g.train(); f.train()\n",
    "        loss_test.append(losst)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 600\n",
    "end = -1\n",
    "\n",
    "fig, axes = plt.subplots(1,3)\n",
    "fig.set_figwidth(18)\n",
    "fig.set_figheight(5)\n",
    "\n",
    "axes[0].set_title('likelihood value')\n",
    "axes[0].plot(loss_train[start:end])\n",
    "axes[0].plot(loss_test[start:end])\n",
    "axes[1].set_title('divergence')\n",
    "axes[1].plot(loss_d[start:end])\n",
    "axes[2].set_title('reconstruction')\n",
    "axes[2].plot(loss_r[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marginal Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "M = 1000\n",
    "B = 125\n",
    "\n",
    "test_single_loader = torch.utils.data.DataLoader(dataset=test_set,batch_size=1,shuffle=False, **kwargs)\n",
    "loglikelihood = []\n",
    "g.eval(); f.eval()\n",
    "for i,x in enumerate(test_single_loader):\n",
    "    if i % 100 == 0: print(i,end=' ')\n",
    "    x = x.cuda()\n",
    "    epsilon = torch.randn(M,1,7,7).cuda()\n",
    "    \n",
    "    samples = []\n",
    "    num_batches = M//B\n",
    "    x = x.expand(B,1,28,28)\n",
    "    for i in range(num_batches):\n",
    "        z, logqzx, _, _ = f(x, epsilon[i*B:(i+1)*B])\n",
    "        logpxz = F.binary_cross_entropy_with_logits(g(z,x), x, reduction='none').view(-1,784).sum(1)\n",
    "        logpz = 0.5*z.pow(2).sum(1)\n",
    "        \n",
    "        samples.extend(-(logpxz + logpz - logqzx).detach().cpu().numpy())\n",
    "    \n",
    "    loglikelihood.append(logsumexp(np.array(samples)) - np.log(M))\n",
    "g.train(); f.train()\n",
    "print(-np.mean(loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
